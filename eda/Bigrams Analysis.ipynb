{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491c8ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tweepy\n",
      "  Downloading tweepy-4.14.0-py3-none-any.whl (98 kB)\n",
      "     -------------------------------------- 98.5/98.5 kB 630.6 kB/s eta 0:00:00\n",
      "Collecting oauthlib<4,>=3.2.0 (from tweepy)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     ------------------------------------ 151.7/151.7 kB 648.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\corts033\\anaconda3\\lib\\site-packages (from tweepy) (2.27.1)\n",
      "Collecting requests-oauthlib<2,>=1.2.0 (from tweepy)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\corts033\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\corts033\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\corts033\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\corts033\\anaconda3\\lib\\site-packages (from requests<3,>=2.27.0->tweepy) (3.3)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, tweepy\n",
      "Successfully installed oauthlib-3.2.2 requests-oauthlib-1.3.1 tweepy-4.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\corts033\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\corts033\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c55a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tweepy as tw\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx as nx\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "309e2ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['culture clash', 'future', 'space war', 'space colony', 'society', 'space travel', 'futuristic', 'romance', 'space', 'alien', 'tribe', 'alien planet', 'cgi', 'marine', 'soldier', 'battle', 'love affair', 'anti war', 'power relations', 'mind and soul', '3d']\n",
      "[\"['ocean'\", \" 'drug abuse'\", \" 'exotic island'\", \" 'east india trading company'\", ' \"love of one\\'s life\"', \" 'traitor'\", \" 'shipwreck'\", \" 'strong woman'\", \" 'ship'\", \" 'alliance'\", \" 'calypso'\", \" 'afterlife'\", \" 'fighter'\", \" 'pirate'\", \" 'swashbuckler'\", \" 'aftercreditsstinger']\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"['culture clash'\", \" 'future'\"),\n",
       " (\" 'future'\", \" 'space war'\"),\n",
       " (\" 'space war'\", \" 'space colony'\"),\n",
       " (\" 'space colony'\", \" 'society'\"),\n",
       " (\" 'society'\", \" 'space travel'\"),\n",
       " (\" 'space travel'\", \" 'futuristic'\"),\n",
       " (\" 'futuristic'\", \" 'romance'\"),\n",
       " (\" 'romance'\", \" 'space'\"),\n",
       " (\" 'space'\", \" 'alien'\"),\n",
       " (\" 'alien'\", \" 'tribe'\"),\n",
       " (\" 'tribe'\", \" 'alien planet'\"),\n",
       " (\" 'alien planet'\", \" 'cgi'\"),\n",
       " (\" 'cgi'\", \" 'marine'\"),\n",
       " (\" 'marine'\", \" 'soldier'\"),\n",
       " (\" 'soldier'\", \" 'battle'\"),\n",
       " (\" 'battle'\", \" 'love affair'\"),\n",
       " (\" 'love affair'\", \" 'anti war'\"),\n",
       " (\" 'anti war'\", \" 'power relations'\"),\n",
       " (\" 'power relations'\", \" 'mind and soul'\"),\n",
       " (\" 'mind and soul'\", \" '3d']\")]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = pd.read_csv('C:/Users/corts033/Documents/Personal/Geo Tech/CSE 6242/Project/CSE6242_Project_main_data_cleaned_movie_and_credits.txt')\n",
    "# Print head\n",
    "papers[\"keywords_list\"].head()\n",
    "\n",
    "key_words = papers[\"keywords_list\"]\n",
    "\n",
    "\n",
    "\n",
    "print(key_words[0])\n",
    "\n",
    "words_in_movie = [i.lower().split(\",\") for i in key_words]\n",
    "#words_in_movie = [i.lower().replace(\"based on\",\"\") for i in key_words]\n",
    "#words_in_movie = [i.lower().replace(\"on\",\"\") for i in key_words]\n",
    "#words_in_movie = [i.lower().replace(\"of\",\"\") for i in key_words]\n",
    "#words_in_movie = [i.lower().replace(\"]\",\"\") for i in key_words]\n",
    "#words_in_movie = [i.lower().replace(\"[\",\"\") for i in key_words]\n",
    "#words_in_movie = [i.lower().replace(\"''\",\"\") for i in key_words]\n",
    "print(words_in_movie[1])\n",
    "\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Remove stop words from each tweet list of words\n",
    "#movie_nsw = [[word for word in words_in_movie if not word in tuple(stop_words)]\n",
    "              #for movie_words in words_in_movie]\n",
    "\n",
    "# Remove collection words\n",
    "#collection_words = ['based', 'on', 'director']\n",
    "\n",
    "#keywords_nsw_nc = [[w for w in word if not w in collection_words]\n",
    "                 #for word in movie_nsw]\n",
    "\n",
    "#print(keywords_nsw_nc[0])\n",
    "# extract subject\n",
    "#source = [i[0] for i in key_words ]\n",
    "\n",
    "# extract object\n",
    "#target = [i[1] for i in key_words ]\n",
    "\n",
    "#relations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'])]\n",
    "\n",
    "#kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "\n",
    "# Create list of lists containing bigrams in tweets\n",
    "terms_bigram = [list(bigrams(i)) for i in words_in_movie]\n",
    "\n",
    "# View bigrams for the first tweet\n",
    "terms_bigram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38e6d22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((\" 'aftercreditsstinger'\", \" 'duringcreditsstinger']\"), 44),\n",
       " ((\" 'superhero'\", \" 'based on comic book'\"), 32),\n",
       " ((\" 'aftercreditsstinger'\", \" 'duringcreditsstinger'\"), 23),\n",
       " ((\" 'revenge'\", \" 'murder'\"), 22),\n",
       " ((\" 'post-apocalyptic'\", \" 'dystopia'\"), 20),\n",
       " ((\" 'marvel comic'\", \" 'superhero'\"), 19),\n",
       " ((\" 'murder'\", \" 'suspense'\"), 16),\n",
       " ((\" 'duringcreditsstinger'\", \" '3d']\"), 12),\n",
       " ((\" 'murder'\", \" 'independent film'\"), 12),\n",
       " ((\" 'explosion'\", \" 'violence'\"), 11),\n",
       " ((\" 'sequel'\", \" 'superhero'\"), 10),\n",
       " ((\"['male nudity'\", \" 'female nudity'\"), 10),\n",
       " ((\"['independent film'\", \" 'woman director']\"), 10),\n",
       " ((\" 'future'\", \" 'dystopia'\"), 9),\n",
       " ((\" 'duringcreditsstinger'\", \" 'woman director']\"), 9),\n",
       " ((\" 'investigation'\", \" 'police'\"), 9),\n",
       " ((\" 'love'\", \" 'friends'\"), 9),\n",
       " ((\" 'space'\", \" 'alien'\"), 8),\n",
       " ((\"['saving the world'\", \" 'artificial intelligence'\"), 8),\n",
       " ((\" 'biography'\", \" 'sport'\"), 8)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten list of bigrams in clean tweets\n",
    "bigrams = list(itertools.chain(*terms_bigram))\n",
    "\n",
    "# Create counter of words in clean bigrams\n",
    "bigram_counts = collections.Counter(bigrams)\n",
    "\n",
    "bigram_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "63017ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df = pd.DataFrame(bigram_counts.most_common(100),\n",
    "                             columns=['bigram', 'count'])\n",
    "\n",
    "bigram_df\n",
    "\n",
    "bigram_df.to_csv('C:/Users/corts033/Documents/Personal/Geo Tech/CSE 6242/Project/CSE6242_Bigram.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4963be16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babf6f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
